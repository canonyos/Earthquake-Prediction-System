# Comprehensive Earthquake Prediction Pipeline Example
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split

# ----- 1. Data Simulation Functions -----

def generate_seismic_data(samples=1000, timesteps=100):
    """
    Generate synthetic seismic signals.
    Each sample is a time-series of seismic readings.
    """
    # Simulate seismic data as random noise with some sinusoidal component indicating tremor
    t = np.linspace(0, 10, timesteps)
    seismic_signals = np.array([np.sin(t + np.random.rand()*2*np.pi) + 0.5 * np.random.randn(timesteps) 
                                for _ in range(samples)])
    # Reshape to (samples, timesteps, 1)
    return seismic_signals.reshape(samples, timesteps, 1)

def generate_muon_data(samples=1000, timesteps=100):
    """
    Generate synthetic muon tomography data.
    This data simulates subsurface density variations.
    """
    # Simulate muon data as random fluctuations (could be from attenuation measurements)
    muon_signals = np.random.rand(samples, timesteps)
    # Optionally add a trend or pattern representing density anomalies
    muon_signals += np.linspace(0, 1, timesteps)  # slight gradient across time
    # Reshape to (samples, timesteps, 1)
    return muon_signals.reshape(samples, timesteps, 1)

def fuse_data(seismic_data, muon_data):
    """
    Concatenate seismic and muon data along the feature dimension.
    New shape becomes (samples, timesteps, 2)
    """
    return np.concatenate([seismic_data, muon_data], axis=-1)

# ----- 2. Data Preparation and Fusion -----

samples = 1000
timesteps = 100

seismic_data = generate_seismic_data(samples, timesteps)
muon_data = generate_muon_data(samples, timesteps)
fused_data = fuse_data(seismic_data, muon_data)

# For simplicity, we simulate labels as binary (0: no quake, 1: quake)
# In real applications, labels will come from historical event data.
labels = np.random.randint(0, 2, (samples, 1))

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(fused_data, labels, test_size=0.2, random_state=42)

# ----- 3. Building the Deep Learning Model (LSTM/CNN-LSTM Hybrid) -----

def create_lstm_model(input_shape):
    model = Sequential([
        LSTM(128, return_sequences=True, input_shape=input_shape),
        Dropout(0.2),
        LSTM(64),
        Dense(1, activation='sigmoid')  # Binary classification output
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Create and train the model
input_shape = (timesteps, 2)  # 2 features from fused data
model = create_lstm_model(input_shape)
model.summary()

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Evaluate the model on test data
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Save the trained model to an HDF5 file (for real-time IoT deployment later)
model.save("earthquake_predictor.h5")

# ----- 4. Federated Learning Integration (Simple Simulation) -----
# For federated learning, we use TensorFlow Federated (TFF).
# Hereâ€™s a simplified example simulating two client datasets.

import tensorflow_federated as tff

# Create a simple Keras model to be used in federated simulation.
def create_compiled_keras_model():
    model = create_lstm_model(input_shape)
    return model

# Define a model function for TFF
def model_fn():
    keras_model = create_compiled_keras_model()
    return tff.learning.from_keras_model(
        keras_model,
        input_spec={
            'x': tf.TensorSpec(shape=(None, timesteps, 2), dtype=tf.float32),
            'y': tf.TensorSpec(shape=(None, 1), dtype=tf.float32)
        },
        loss=tf.keras.losses.BinaryCrossentropy(),
        metrics=[tf.keras.metrics.Accuracy()]
    )

# Simulate two sets of client data
client_data_1 = tf.data.Dataset.from_tensor_slices({'x': X_train[:400], 'y': y_train[:400]}).batch(32)
client_data_2 = tf.data.Dataset.from_tensor_slices({'x': X_train[400:800], 'y': y_train[400:800]}).batch(32)
federated_train_data = [client_data_1, client_data_2]

# Build the federated averaging process
federated_averaging = tff.learning.build_federated_averaging_process(model_fn,
                                                                     client_optimizer_fn=lambda: tf.keras.optimizers.Adam(0.001),
                                                                     server_optimizer_fn=lambda: tf.keras.optimizers.Adam(0.001))
state = federated_averaging.initialize()

# Run a few rounds of federated training simulation
for round_num in range(1, 6):
    state, metrics = federated_averaging.next(state, federated_train_data)
    print(f"Round {round_num}, Metrics: {metrics}")

# ----- End of Comprehensive Pipeline -----